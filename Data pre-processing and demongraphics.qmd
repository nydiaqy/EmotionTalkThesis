---
title: "Data Pre-processing and Demographics"
output: html_document
---

```{r setup, include=FALSE}
 knitr::opts_chunk$set( echo = FALSE, # hide code by default 
                        message = FALSE, # hide messages 
                        warning = FALSE, # hide warnings 
                        results = "hide" # hide printed output from code 
                        )

```

# Data pre-processing and demographics

Prep work space and load packages

```{r}
rm(list=ls())
library(childesr)
library(dplyr)
library(wordcloud)
library(ggplot2)
library(tidyr)
library(purrr)
library(stringr)
library(quanteda)
library(irlba)
library(spacyr)



```

Load CHILDES transcripts/ utterance dataframe in English-North America

```{r,echo=TRUE, results="show"}
d_eng_na <- get_transcripts(collection = "Eng-NA")
head(d_eng_na)
count_transcripts <- nrow(d_eng_na)
count_transcripts

ut_eng_na <- get_utterances(collection = "Eng-NA")
head(ut_eng_na)
```

Load participant information: Number of participants, transcripts, utterances by roles

```{r,echo=TRUE, results="show"}

count_by_role_check <- ut_eng_na %>% 
  count(speaker_code, sort = TRUE) %>%
  print(n = 30)

count_by_role_check
count_by_role

count_by_role <- ut_eng_na %>%
  group_by(speaker_role) %>%
  summarise(
  n_speakers   = n_distinct(speaker_id),
  n_transcripts = n_distinct(transcript_id),
  n_utterances = n(),
    .groups = "drop"
  ) 
#Order by number of transcripts
count_by_role <- count_by_role %>%
  arrange(desc(n_transcripts)) %>%
  mutate(
    speaker_role = factor(speaker_role,
                          levels = speaker_role)
  )

count_by_role

count_by_role_exc_child <- count_by_role %>% 
  filter(speaker_role != "Target_Child")

count_by_role_exc_child
```

Pie Chart showing number of transcripts by role excluding target child

```{r}
ggplot(count_by_role_exc_child,
       aes(x = "", y = n_transcripts, fill = speaker_role)) +
  geom_col(width = 1) +
  coord_polar(theta = "y") +
  theme_void() +
  labs(title = "Distribution of Transcripts by Role")
```

### Group conversational partners

Grouping based on Bronfenbrenner’s ecological systems

Group 1: Mother (MOT)

Group 2: Father (FAT)

Group 3: Adult with exisitng relationship (ADT): Grandmother, Grandfather, Relative, Caretaker, Caregiver, Teacher

Group 4: Siblings (SIB): Sister, Brother, Sibling

Group 5: Other children (OTH_C): Child, Friend, Playmate, Student, Girl, Teenager

Group 6: Other adults (Exosystem) (OTH_A) : Investigator, Media, Unidentified, Adult, Visitor, Participants, Environment, Male, Uncertain

```{r}
role_category_map <- tibble(
  speaker_role = c(
    # two parents 
    "Mother","Father",
    
    #adult with exsiting relationship (exp parents)
  "Grandmother", "Grandfather", "Relative", "Caretaker", "Caregiver","Teacher",
    
    #siblings 
    "Sister", "Brother", 
    "Sibling",
    
    #other child 
    "Child", "Friend", "Playmate", "Student", "Girl", "Teenager",
    
    #other adults
    "Unidentified", "Adult", "Media", "Visitor",
    "Participant", "Environment", "Male", "Uncertain","Investigator"
  ),
  role_category = c(
    #group 1 and 2: keep parents as they are
    "MOT", "FAT",
    
    #group 3: adults with relationship
    rep("ADT", 6),
    
    #group 4: sibling
    rep("SIB", 3),
    
    #group 5: other children 
    rep ("OTH_C", 6),
    
    #group 6: other adult
    rep("OTH_A", 9)
  )
)

count_by_role_exc_child <- count_by_role_exc_child %>%
  left_join(role_category_map, by = "speaker_role")
count_by_role <- count_by_role %>% 
  left_join(role_category_map, by = "speaker_role")

count_by_role_exc_child

#Sanity check 
count_by_role_exc_child %>%
  filter(is.na(role_category)) %>%
  distinct(speaker_role)
```

Number of participant, transcripts, utterance by each category

```{r}
count_by_category <- count_by_role_exc_child %>%
  group_by(role_category) %>%
  summarise(
    n_speakers    = sum(n_speakers),
    n_transcripts = sum(n_transcripts),
    n_utterances  = sum(n_utterances),
    .groups = "drop"
  ) %>%
  arrange(desc(n_transcripts))

count_by_category
```

Load information on the target child: number of target child, sex, age

```{r,echo=TRUE, results="show"}
d_target_child <- get_participants(role = "target_child", collection = "Eng-NA")
count_by_role

sum_sex <- d_target_child %>%
  count(sex)
sum_sex

sum_age <-d_target_child %>%
  summarise(
    mean_age = mean(min_age, na.rm = TRUE),
    sd_age = sd(min_age, na.rm = TRUE),
    min_age = min(min_age, na.rm = TRUE),
    max_age = max(max_age, na.rm = TRUE)
  )
sum_age
```

Load information about the SES and education

```{r,echo=TRUE, results="show"}
sum_ses <- d_target_child %>%
  count(ses)
sum_ses

sum_education <-d_target_child %>%
  count(education)
sum_education
```

# RQ1

Identify if any transcripts, only the target child is speaking (no conversational partner speech)

```{r}
# Identify only-child transcripts
onlychild_ids <- ut_eng_na %>%
  group_by(transcript_id) %>%
  summarise(only_child = all(speaker_code == "CHI")) %>%
  filter(only_child == TRUE) %>%
  pull(transcript_id)

#transcript:
eng_na_rq1 <- d_eng_na %>%
  filter(!transcript_id %in% onlychild_ids)
head(eng_na_rq1)

count_transcripts_rq1 <- nrow(eng_na_rq1)
count_transcripts_rq1
#number of transcripts where there is a conversational partner
```

Pull out all conversational partner's turns

```{r}
ut_eng_na_rq1 <- ut_eng_na %>%
  filter(!transcript_id %in% onlychild_ids) %>%   
  filter(speaker_role != "Target_Child") %>% 
  left_join(role_category_map, by = "speaker_role")      #add new role category
```

Create target word list

```{r}
file_lex <- read.csv("~/Desktop/R_WD/ThesisDataAnalysis/1987-Affectivelexicon-foundations_words.csv",
                stringsAsFactors = FALSE)
emotion_words <-file_lex$Word
emotion_words <- unique(na.omit(emotion_words))
head(emotion_words)
length(emotion_words)
```

identify how many emotion words each utterance used

```{r}
utt_partners_emotions <- ut_eng_na_rq1 %>%
  mutate(
    gloss_lower = tolower(gloss),
    emotion_count = stringr::str_count(
      gloss_lower, 
      paste0("\\b(", paste(emotion_words, collapse = "|"), ")\\b")
    )
  )

gloss_vec <- utt_partners_emotions$gloss_lower
emotion_cols <- map_dfc(emotion_words, ~ {
  str_count(gloss_vec, paste0("\\b", .x, "\\b"))
}) %>%
  setNames(emotion_words)   # name columns with the words

# Bind those new columns back onto your utterance-level data
utt_partners_emotions <- bind_cols(utt_partners_emotions, emotion_cols)
head(utt_partners_emotions)


```

Across all transcripts, how many times each words are used + frequency

```{r}
total_emotion_words <- utt_partners_emotions %>%
  summarise(across(all_of(emotion_words), sum, na.rm = TRUE))
#get raw totals


emotion_counts_long <- total_emotion_words %>%
  pivot_longer(
    cols = everything(),
    names_to = "word",
    values_to = "count"
  ) %>%
  mutate(freq_per_1000 = (count / sum(count))*1000)
#convert to long + compute frequency 

emotion_counts_long
```

Save core objects before next step

```{r}
save(
  utt_partners_emotions,
  file = "Emotion word use per utterance_RQ1"
)
```

## Problem 1. Disambiguation. e.g. like, well, kind.

### Step 1: Flag ambiguous words from main emotion word list 

### Step 2 :Subset utterances containing those words

get a list of the 100 most used emotion words (bc more likely to be used for multiple reasons + affect the result )

```{r}
top100_words <- emotion_counts_long %>%
  arrange(desc(count)) %>%
  slice(1:100)

ggplot(top100_words, aes(x = reorder(word, count), 
                        y = count)) +
  geom_col(fill = "steelblue") +
  geom_text(aes(label = count),
            hjust = -0.1, size = 3) +     # <- adds labels
  coord_flip() +
  theme_minimal() +
  labs(
    x = "Emotion Word",
    y = "Total Count",
    title = "Top 100 Most Frequent Emotion Words"
  ) +
  ylim(0, max(top100_words$count) * 1.1)  # extra space for labels

top100_words
```

Sample up to 20 example utterance each word

```{r}
top100_words <- tolower(top100_words$word)


examples_100word <- map_dfr(top100_words, function(w) {
  
  # all utterances where this word appears at least once
  df_w <- utt_partners_emotions %>%
    filter(.data[[w]] > 0)   # use the column named w
  
  # if no utterances contain this word, skip it
  if (nrow(df_w) == 0) return(NULL)
  
  # how many to sample (max 20, but not more than we have)
  n_samp <- min(20L, nrow(df_w))
  
  df_w %>%
    slice_sample(n = n_samp) %>%
    mutate(target_word = w)
}) 

write.csv(
  examples_100word,
  "top100_words_20examples.csv",
  row.names = FALSE
)
```

Handpick - create a list of ambiguous words

```{r}
ambiguous_words <- c(
  "like","well","kind","blue","fine","high","lost","quiet","sick",
  "strong","gentle","afraid","merry","moved","low","certain","sore",
  "touched","patient","odd","rotten","pride","alarm"
)

ambiguous_cols <- intersect(ambiguous_words, colnames(utt_partners_emotions))
ambiguous_cols

amb_utt <- utt_partners_emotions %>%
  filter(if_any(all_of(ambiguous_cols), ~ .x > 0)) %>% mutate(
    ambiguous_word = apply(
      select(., all_of(ambiguous_cols)),
      1,
      function(x) {
        paste(ambiguous_cols[x > 0], collapse = "|")
      }
    )
  ) %>% 
  select(ambiguous_word, id, gloss,role_category,gloss_lower)
head(amb_utt)

#sanity check
amb_utt %>%
  count(ambiguous_word, sort = TRUE)
```

### Step 3: POS and dependency-based disambiguation (spaCy)

Load spacy

```{r}
spacy_install()

spacy_initialize(model = "en_core_web_sm")
```

#### 3.1 Like

new dataframe containing only like-included utterances + original utterance id.

```{r}
df_like <- amb_utt %>%
  filter(str_detect(gloss_lower, "\\blike\\b")) %>%
  transmute(doc_id = as.character(id), text = gloss_lower)

x <- df_like$text
names(x) <- df_like$doc_id 

tok <- spacy_parse(
  x,
  lemma = TRUE,
  pos = TRUE,
  dependency = TRUE
)

```

define the rules for like: POS = verb + exclude "look(s) like“

```{r}
like_tok <- tok %>%
  filter(lemma == "like") %>%
  select(doc_id, token_id, lemma, pos, dep_rel, head_token_id)

# Get the head token text (helps remove "look(s) like")
heads <- tok %>%
  select(doc_id, token_id, head_token = token)

#Verb docs
like_emotion_docs <- like_tok %>%
  filter(pos == "VERB") %>%
  left_join(heads, by = c("doc_id", "head_token_id" = "token_id")) %>%
  # exclude comparative construction: look(s) like
  filter(!(head_token %in% c("look","looks","looked","looking"))) %>%
  distinct(doc_id)

#preference doc 
pref_docs <- amb_utt %>%
  transmute(doc_id = as.character(id),
            text = gloss_lower) %>%
  filter(
    # preference questions / offers
    str_detect(text, "\\b(would|could|can|do|did)\\s+you\\s+like\\b") |
    # statements like "I'd like..." (optional but often relevant)
    str_detect(text, "\\b(i\\s*'d|i\\s+would|i\\s+could)\\s+like\\b")
  ) %>%
  distinct(doc_id)

like_emotion_docs <- bind_rows(like_emotion_docs, pref_docs) %>%
  distinct(doc_id)

like_pos_by_doc <- tok %>%
  filter(lemma == "like") %>%
  mutate(doc_id = as.character(doc_id)) %>%
  group_by(doc_id) %>%
  summarise(
    like_pos = paste(sort(unique(pos)), collapse = "|"),
    .groups = "drop"
  )


amb_utt_like_flagged <- amb_utt %>%
  mutate(doc_id = as.character(id)) %>%
  left_join(
    mutate(like_emotion_docs, like_is_emotion = TRUE),
    by = "doc_id"
  ) %>%
  left_join(
    like_pos_by_doc,
    by = "doc_id"
  ) %>%
  mutate(
    like_is_emotion = if_else(is.na(like_is_emotion), FALSE, like_is_emotion)
  )

head(amb_utt_like_flagged)

set.seed(1)

amb_utt_like_flagged %>%
  filter(str_detect(gloss_lower, "\\blike\\b"), like_is_emotion) %>%
  slice_sample(n = 40) %>%
  select(id, gloss)

#check non-emotions exclusion
amb_utt_like_flagged %>%
  filter(str_detect(gloss_lower, "\\blike\\b"), !like_is_emotion) %>%
  slice_sample(n = 30) %>%
  select(id, gloss)

amb_utt_like_flagged %>%
  filter(str_detect(gloss_lower, "\\blike\\b"),
         like_is_emotion == FALSE) %>%
  mutate(
    looks_like = str_detect(gloss_lower, "\\blooks?\\s+like\\b"),
    kind_of    = str_detect(gloss_lower, "\\bkind\\s+of\\s+like\\b")
  ) %>%
  count(like_pos, sort = TRUE)

amb_utt_like_flagged %>%
  filter(id == 2613441)
```

Sanity check:

```{r}
id_to_check <- "3778"   # replace with a transcript id

transcript_check <- utt_partners_emotions %>%
  filter(transcript_id == id_to_check) %>%
  select(transcript_id, speaker_code, gloss, emotion_count)

transcript_check <- transcript_check %>%
  filter(emotion_count > 0)

transcript_check
```

Word counts: all partner

```{r}
ut_eng_na_rq1 <- ut_eng_na_rq1 %>%
  mutate(
    word_count = stringr::str_count(gloss, "\\S+")  # counts non-space sequences = words
  )

partner_word_counts <- ut_eng_na_rq1 %>%
  group_by(transcript_id) %>%
  summarise(
    total_partner_words = sum(word_count, na.rm = TRUE),
    total_partner_utterances = n()
  )
```

Questions:

-   would you like one - counted as non-emotion in this version (ADP) as it appeared in a polite preference question

-   how can i be fully confident that I did not exclude other uses of "like" that may broadly refer to emotion?
