---
title: "Data Pre-processing and Demographics"
output: html_document
---

```{r setup, include=FALSE}
 knitr::opts_chunk$set( echo = FALSE, # hide code by default 
                        message = FALSE, # hide messages 
                        warning = FALSE, # hide warnings 
                        results = "hide" # hide printed output from code 
                        )

```

# Data pre-processing and demographics

Prep work space and load packages

```{r}
rm(list=ls())
library(childesr)
library(dplyr)
library(wordcloud)
library(ggplot2)
library(tidyr)
library(purrr)
library(stringr)
library(quanteda)
library(irlba)



```

Load CHILDES transcripts/ utterance dataframe in English-North America

```{r,echo=TRUE, results="show"}
d_eng_na <- get_transcripts(collection = "Eng-NA")
head(d_eng_na)
count_transcripts <- nrow(d_eng_na)
count_transcripts

ut_eng_na <- get_utterances(collection = "Eng-NA")
head(ut_eng_na)
```

Load participant information: Number of participants, transcripts, utterances by roles

```{r,echo=TRUE, results="show"}

count_by_role_check <- ut_eng_na %>% 
  count(speaker_code, sort = TRUE) %>%
  print(n = 30)

count_by_role_check
count_by_role

count_by_role <- ut_eng_na %>%
  group_by(speaker_role) %>%
  summarise(
  n_speakers   = n_distinct(speaker_id),
  n_transcripts = n_distinct(transcript_id),
  n_utterances = n(),
    .groups = "drop"
  ) 
#Order by number of transcripts
count_by_role <- count_by_role %>%
  arrange(desc(n_transcripts)) %>%
  mutate(
    speaker_role = factor(speaker_role,
                          levels = speaker_role)
  )

count_by_role

count_by_role_exc_child <- count_by_role %>% 
  filter(speaker_role != "Target_Child")

count_by_role_exc_child
```

Pie Chart showing number of transcripts by role excluding target child

```{r}
ggplot(count_by_role_exc_child,
       aes(x = "", y = n_transcripts, fill = speaker_role)) +
  geom_col(width = 1) +
  coord_polar(theta = "y") +
  theme_void() +
  labs(title = "Distribution of Transcripts by Role")
```

### Group conversational partners

Grouping based on Bronfenbrenner’s ecological systems

Group 1: Mother (MOT)

Group 2: Father (FAT)

Group 3: Adult with exisitng relationship (ADT): Grandmother, Grandfather, Relative, Caretaker, Caregiver, Teacher

Group 4: Siblings (SIB): Sister, Brother, Sibling

Group 5: Other children (OTH_C): Child, Friend, Playmate, Student, Girl, Teenager

Group 6: Other adults (Exosystem) (OTH_A) : Investigator, Media, Unidentified, Adult, Visitor, Participants, Environment, Male, Uncertain

```{r}
role_category_map <- tibble(
  speaker_role = c(
    # two parents 
    "Mother","Father",
    
    #adult with exsiting relationship (exp parents)
  "Grandmother", "Grandfather", "Relative", "Caretaker", "Caregiver","Teacher",
    
    #siblings 
    "Sister", "Brother", 
    "Sibling",
    
    #other child 
    "Child", "Friend", "Playmate", "Student", "Girl", "Teenager",
    
    #other adults
    "Unidentified", "Adult", "Media", "Visitor",
    "Participant", "Environment", "Male", "Uncertain","Investigator"
  ),
  role_category = c(
    #group 1 and 2: keep parents as they are
    "MOT", "FAT",
    
    #group 3: adults with relationship
    rep("ADT", 6),
    
    #group 4: sibling
    rep("SIB", 3),
    
    #group 5: other children 
    rep ("OTH_C", 6),
    
    #group 6: other adult
    rep("OTH_A", 9)
  )
)

count_by_role_exc_child <- count_by_role_exc_child %>%
  left_join(role_category_map, by = "speaker_role")
count_by_role <- count_by_role %>% 
  left_join(role_category_map, by = "speaker_role")

count_by_role_exc_child

#Sanity check 
count_by_role_exc_child %>%
  filter(is.na(role_category)) %>%
  distinct(speaker_role)
```

Number of participant, transcripts, utterance by each category

```{r}
count_by_category <- count_by_role_exc_child %>%
  group_by(role_category) %>%
  summarise(
    n_speakers    = sum(n_speakers),
    n_transcripts = sum(n_transcripts),
    n_utterances  = sum(n_utterances),
    .groups = "drop"
  ) %>%
  arrange(desc(n_transcripts))

count_by_category
```

Load information on the target child: number of target child, sex, age

```{r,echo=TRUE, results="show"}
d_target_child <- get_participants(role = "target_child", collection = "Eng-NA")
count_by_role

sum_sex <- d_target_child %>%
  count(sex)
sum_sex

sum_age <-d_target_child %>%
  summarise(
    mean_age = mean(min_age, na.rm = TRUE),
    sd_age = sd(min_age, na.rm = TRUE),
    min_age = min(min_age, na.rm = TRUE),
    max_age = max(max_age, na.rm = TRUE)
  )
sum_age
```

Load information about the SES and education

```{r,echo=TRUE, results="show"}
sum_ses <- d_target_child %>%
  count(ses)
sum_ses

sum_education <-d_target_child %>%
  count(education)
sum_education
```

# RQ1

Identify if any transcripts, only the target child is speaking (no conversational partner speech)

```{r}
# Identify only-child transcripts
onlychild_ids <- ut_eng_na %>%
  group_by(transcript_id) %>%
  summarise(only_child = all(speaker_code == "CHI")) %>%
  filter(only_child == TRUE) %>%
  pull(transcript_id)

#transcript:
eng_na_rq1 <- d_eng_na %>%
  filter(!transcript_id %in% onlychild_ids)
head(eng_na_rq1)

count_transcripts_rq1 <- nrow(eng_na_rq1)
count_transcripts_rq1
#number of transcripts where there is a conversational partner
```

Pull out all conversational partner's turns

```{r}
ut_eng_na_rq1 <- ut_eng_na %>%
  filter(!transcript_id %in% onlychild_ids) %>%   
  filter(speaker_role != "Target_Child") %>% 
  left_join(role_category_map, by = "speaker_role")      #add new role category
```

Create target word list

```{r}
file_lex <- read.csv("~/Desktop/R_WD/ThesisDataAnalysis/1987-Affectivelexicon-foundations_words.csv",
                stringsAsFactors = FALSE)
emotion_words <-file_lex$Word
emotion_words <- unique(na.omit(emotion_words))
head(emotion_words)
length(emotion_words)
```

identify how many emotion words each utterance used

```{r}
utt_partners_emotions <- ut_eng_na_rq1 %>%
  mutate(
    gloss_lower = tolower(gloss),
    emotion_count = stringr::str_count(
      gloss_lower, 
      paste0("\\b(", paste(emotion_words, collapse = "|"), ")\\b")
    )
  )

gloss_vec <- utt_partners_emotions$gloss_lower
emotion_cols <- map_dfc(emotion_words, ~ {
  str_count(gloss_vec, paste0("\\b", .x, "\\b"))
}) %>%
  setNames(emotion_words)   # name columns with the words

# Bind those new columns back onto your utterance-level data
utt_partners_emotions <- bind_cols(utt_partners_emotions, emotion_cols)
head(utt_partners_emotions)


```

Across all transcripts, how many times each words are used + frequency

```{r}
total_emotion_words <- utt_partners_emotions %>%
  summarise(across(all_of(emotion_words), sum, na.rm = TRUE))
#get raw totals


emotion_counts_long <- total_emotion_words %>%
  pivot_longer(
    cols = everything(),
    names_to = "word",
    values_to = "count"
  ) %>%
  mutate(freq_per_1000 = (count / sum(count))*1000)
#convert to long + compute frequency 

emotion_counts_long
```

Save core objects before next step

```{r}
save(
  utt_partners_emotions,
  file = "Emotion word use per utterance_RQ1"
)
```

#### Problem 1. some of the words are not used as emotion words. e.g. like, well, kind.

#### Method: human-in-the-loop NLP filter

##### Step 1: Flag ambiguous words from main emotion word list

get a list of the 100 most used emotion words (bc more likely to be used for multiple reasons + affect the result )

```{r}
top100_words <- emotion_counts_long %>%
  arrange(desc(count)) %>%
  slice(1:100)

ggplot(top100_words, aes(x = reorder(word, count), 
                        y = count)) +
  geom_col(fill = "steelblue") +
  geom_text(aes(label = count),
            hjust = -0.1, size = 3) +     # <- adds labels
  coord_flip() +
  theme_minimal() +
  labs(
    x = "Emotion Word",
    y = "Total Count",
    title = "Top 100 Most Frequent Emotion Words"
  ) +
  ylim(0, max(top100_words$count) * 1.1)  # extra space for labels

top100_words
```

I need to tackle the ambiguous words by following steps

1.  manually check what are the utterances looks like and what are the words that might not be emotion words in that context. Sample up to 20 example utterance each word

```{r}
top100_words <- tolower(top100_words$word)


examples_100word <- map_dfr(top100_words, function(w) {
  
  # all utterances where this word appears at least once
  df_w <- utt_partners_emotions %>%
    filter(.data[[w]] > 0)   # use the column named w
  
  # if no utterances contain this word, skip it
  if (nrow(df_w) == 0) return(NULL)
  
  # how many to sample (max 20, but not more than we have)
  n_samp <- min(20L, nrow(df_w))
  
  df_w %>%
    slice_sample(n = n_samp) %>%
    mutate(target_word = w)
}) 

write.csv(
  examples_100word,
  "top100_words_20examples.csv",
  row.names = FALSE
)
```

Handpick - create a list of ambiguous words (if possible, cross-reference with LIWC)

```{r}
ambiguous_words <- c(
  "like","well","kind","blue","fine","high","lost","quiet","sick",
  "strong","gentle","afraid","merry","moved","low","certain","sore",
  "touched","patient","odd","rotten","pride","alarm"
)

ambiguous_cols <- intersect(ambiguous_words, colnames(utt_partners_emotions))
ambiguous_cols

amb_utt <- utt_partners_emotions %>%
  filter(if_any(all_of(ambiguous_cols), ~ .x > 0))
head(amb_utt)
```

##### Step 2: Subset utterances containing those words

```{r}
amb_pat <- paste0("\b(", paste(ambiguous_words, collapse="|"), ")\\b")

amb_utt <- ut_eng_na_rq1 %>% 
  mutate(gloss_lower = str_to_lower(gloss)) %>% 
  filter(str_detect(gloss_lower, amb_pat)) %>% 
  mutate(matches = str_extract_all(gloss_lower, amb_pat)) %>% 
  unnest_longer(matches, values_to = "amb_word")
```

##### Step 3: Use NLP to filter contexts that are “not emotion talk”

##### Based on windowed utterances/ all ambiguous words together

```{r}

# extract context windows for one utterance ---
extract_windows_one <- function(gloss, id, k = 3, ambiguous_words) {
  toks <- unlist(str_split(gloss, "\\s+"))
  if (length(toks) == 0) return(tibble())
  
  hit_pos <- which(toks %in% ambiguous_words)
  if (length(hit_pos) == 0) return(tibble())
  
  map_dfr(hit_pos, function(p) {
    left  <- max(1, p - k)
    right <- min(length(toks), p + k)
    tibble(
      id      = id,
      amb_word    = toks[p],
      position    = p,
      window_k    = k,
      window_text = paste(toks[left:right], collapse = " "),
      left_context  = paste(toks[left:(p-1)], collapse = " "),
      right_context = paste(toks[(p+1):right], collapse = " ")
    )
  })
}

# --- Apply to all utterances ---
k <- 3  # change to 4/5 later
windows_df <- pmap_dfr(
  list(amb_utt$gloss, amb_utt$id),
  ~ extract_windows_one(..1, ..2, k = k, ambiguous_words = ambiguous_words)
)

# Join back original utterance text if you want
windows_df <- windows_df %>%
  left_join(amb_utt %>% select(id, gloss), by = "id")

```

Run clustering based on windowed text

```{r}
corp <- corpus(windows_df$window_text)

toks <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("en"))

dfm_mat <- dfm(toks) %>%
  dfm_trim(min_docfreq = 3)

dfm_tfidf <- dfm_tfidf(dfm_mat)
X <- as.matrix(dfm_tfidf)

svd_out <- irlba(X, nv = 50)
emb <- svd_out$u %*% diag(svd_out$d)

set.seed(123)
k_clusters <- 6
km <- kmeans(emb, centers = k_clusters, nstart = 20)

windows_df$cluster <- km$cluster

```

Inspect clusters

```{r}
inspect_clusters<-windows_df %>%
  group_by(cluster) %>%
  slice_sample(n = 20) %>%
  ungroup() %>%
  select(cluster, amb_word, window_text, gloss)
inspect_clusters

write.csv(inspect_clusters, "ambiguous_windows_clustered.csv", row.names = FALSE)

```

##### Based on utterances/ all ambiguous words together

```{r}
# Use whichever text column you have
texts <- amb_utt$gloss

corp <- corpus(texts)

toks <- tokens(
  corp,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_numbers = TRUE
) %>%
  tokens_tolower()

# Optional: remove stopwords (often helps), but keep "not" if you care about negation
toks <- tokens_remove(toks, pattern = stopwords("en"))

dfm_mat <- dfm(toks)

# Drop very rare terms (tune min_docfreq if needed)
dfm_mat <- dfm_trim(dfm_mat, min_docfreq = 5)

# TF–IDF weighting
dfm_tfidf <- dfm_tfidf(dfm_mat)

```

```{r}

X <- as.matrix(dfm_tfidf)

# SVD to reduce noise; 50–200 dims is typical
svd_out <- irlba(X, nv = 100)
emb <- svd_out$u %*% diag(svd_out$d)  # utterance embeddings

set.seed(123)
k <- 5  # try 4–8
km <- kmeans(emb, centers = k, nstart = 20)

amb_utt$cluster <- km$cluster

```

```{r, echo=TRUE, results="show"}
top_terms <- lapply(1:k, function(c){
  rows <- which(amb_utt$cluster == c)
  colMeans(X[rows, , drop = FALSE]) %>%
    sort(decreasing = TRUE) %>%
    head(15)
})

top_terms

set.seed(1)
cluster <- amb_utt %>%
  group_by(cluster) %>%
  slice_sample(n = 20) %>%
  ungroup() %>%
  select(cluster, gloss)
cluster
write.csv(cluster, "ambiguous_utterance_clustered.csv", row.names = FALSE)


```

clustering word by word

```{r}
nlp_target <- "like" 
k <- 3   

windows_target <- windows_df %>%
  filter(amb_word == nlp_target)

nrow(windows_target)

corp <- corpus(windows_target$window_text)

toks <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(setdiff(stopwords("en"), "not"))  # keep negation

dfm_mat <- dfm(toks) %>%
  dfm_trim(min_docfreq = 3)

dfm_tfidf <- dfm_tfidf(dfm_mat)
X <- as.matrix(dfm_tfidf)

svd_out <- irlba(X, nv = 30)   # short windows → small nv is enough
emb <- svd_out$u %*% diag(svd_out$d)

set.seed(123)
k_clusters <- 4                # try 3–5
km <- kmeans(emb, centers = k_clusters, nstart = 20)

windows_target$cluster <- km$cluster

set.seed(1)
like_clustered <- windows_target %>%
  group_by(cluster) %>%
  slice_sample(n = 25) %>%
  ungroup() %>%
  select(cluster, window_text, gloss)
like_clustered


like_top_terms <- lapply(sort(unique(windows_target$cluster)), function(c){
  rows <- which(windows_target$cluster == c)
  term_means <- colMeans(X[rows, , drop = FALSE])
  tibble(
    cluster = c,
    term = names(sort(term_means, decreasing = TRUE))[1:12],
    score = sort(term_means, decreasing = TRUE)[1:12]
  )
}) %>% bind_rows()
like_top_terms

write.csv(like_clustered, "like_clustered.csv", row.names = FALSE)
write.csv(like_top_terms, "like_top_terms.csv", row.names = FALSE)


```

reviewing the clustered and top-terms, now my model is being hijacked by "looks like" and "look like" which is obviously not an emotion word.

```{r}
look_like_count <- windows_target %>%
  mutate(window_text_l = stringr::str_to_lower(window_text)) %>%
  summarise(
    n_look_like = sum(stringr::str_detect(window_text_l, "\\blooks?\\s+like\\b")),
    total_windows = n(),
    proportion = n_look_like / total_windows
  )

look_like_count

#broken down by cluster
look_like_by_cluster <- windows_target %>%
  mutate(window_text_l = str_to_lower(window_text)) %>%
  group_by(cluster) %>%
  summarise(
    n_windows = n(),
    n_look_like = sum(str_detect(window_text_l, "\\blooks?\\s+like\\b")),
    prop_look_like = n_look_like / n_windows,
    .groups = "drop"
  )

look_like_by_cluster
```

in cluster 2 and 3, most of the utterances are containing "look like" or "looks like". so I am going to exclude it.

```{r}
windows_like_clean <- windows_target %>%
  mutate(window_text_l = str_to_lower(window_text)) %>%
  filter(!str_detect(window_text_l, "\\blooks?\\s+like\\b"))
nrow(windows_like_clean)

corp <- corpus(windows_like_clean$window_text)

#remove um from the utterances (not removing utterances including um)
toks <- tokens(corpus(windows_like_clean$window_text),
               remove_punct = TRUE,
               remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(c(setdiff(stopwords("en"), "not"), "um"))

dfm_mat <- dfm(toks) %>%
  dfm_trim(min_docfreq = 3)

dfm_tfidf <- dfm_tfidf(dfm_mat)
X <- as.matrix(dfm_tfidf)

svd_out <- irlba(X, nv = 30)
emb <- svd_out$u %*% diag(svd_out$d)

set.seed(123)
k_clusters <- 3   # start with 3 now that look-like is gone
km <- kmeans(emb, centers = k_clusters, nstart = 20)

windows_like_clean$cluster <- km$cluster
set.seed(1)
like_clean_clustered <- windows_like_clean %>%
  group_by(cluster) %>%
  slice_sample(n = 25) %>%
  ungroup() %>%
  select(cluster, window_text, gloss)

like_clean_top_terms <- lapply(sort(unique(windows_like_clean$cluster)), function(c){
  rows <- which(windows_like_clean$cluster == c)
  term_means <- colMeans(X[rows, , drop = FALSE])
  tibble(
    cluster = c,
    term = names(sort(term_means, decreasing = TRUE))[1:12],
    score = sort(term_means, decreasing = TRUE)[1:12]
  )
}) %>% bind_rows()

write.csv(like_clean_clustered, "like_clean_clustered.csv", row.names = FALSE)
write.csv(like_clean_top_terms, "like_clean_top_terms.csv", row.names = FALSE)


```

-\> each word might need different method.

e.g. Part of Speech tagging (well, like) , rule based exclusion (kind of), machine learning (?)

Produce a manual to tag the ambiguous words?

emotion_counts \<- utt_partners_emotions %\>% group_by(transcript_id) %\>% summarise( total_emotion_words = sum(emotion_count, na.rm = TRUE), total_emotion_utterances = sum(emotion_count \> 0) )

Sanity check:

```{r}
id_to_check <- "3778"   # replace with a transcript id

transcript_check <- utt_partners_emotions %>%
  filter(transcript_id == id_to_check) %>%
  select(transcript_id, speaker_code, gloss, emotion_count)

transcript_check <- transcript_check %>%
  filter(emotion_count > 0)

transcript_check
```

Word counts: all partner

```{r}
ut_eng_na_rq1 <- ut_eng_na_rq1 %>%
  mutate(
    word_count = stringr::str_count(gloss, "\\S+")  # counts non-space sequences = words
  )

partner_word_counts <- ut_eng_na_rq1 %>%
  group_by(transcript_id) %>%
  summarise(
    total_partner_words = sum(word_count, na.rm = TRUE),
    total_partner_utterances = n()
  )
```
